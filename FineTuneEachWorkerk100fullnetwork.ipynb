{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code used to fine-tune the whole network for each worker in the test set, when k=100\n",
    "\n",
    "3-fold validation setting\n",
    "\n",
    "We load the mean aesthetic predictor network, and for each worker, we select 100 images to fine-tune the network with. This is done 3 times and results, in spearman and MSE, are averaged for the 3 validations for each worker. We save the results in a file. We do not save any network at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import PIL\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "from random import randint\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath('')\n",
    "data_dir = os.path.join(root_dir, 'Images')\n",
    "# check for existence\n",
    "os.path.exists(root_dir)\n",
    "os.path.exists(data_dir)\n",
    "workers_test = pd.read_csv(os.path.join(data_dir,  'test_workers_dataset_normalized.csv'), sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "class ImageRatingsDataset(Dataset):\n",
    "    \"\"\"Images dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.images_frame = pd.read_csv(csv_file, sep=' ')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_name = str(os.path.join(self.root_dir,str(self.images_frame.iloc[idx, 0])))\n",
    "            im = Image.open(img_name).convert('RGB')\n",
    "            if im.mode == 'P':\n",
    "                im = im.convert('RGB')\n",
    "            image = np.asarray(im)\n",
    "            #image = io.imread(img_name+'.jpg', mode='RGB').convert('RGB')\n",
    "            rating = self.images_frame.iloc[idx, 1]\n",
    "            sample = {'image': image, 'rating': rating}\n",
    "\n",
    "            if self.transform:\n",
    "                sample = self.transform(sample)\n",
    "            return sample            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        image = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        return {'image': image, 'rating': rating}\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "\n",
    "        return {'image': image, 'rating': rating}\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        if random.random() < self.p:\n",
    "            image = np.flip(image, 1)\n",
    "            #image = io.imread(img_name+'.jpg', mode='RGB').convert('RGB')\n",
    "        return {'image': image, 'rating': rating}\n",
    "\n",
    "class Normalize(object):\n",
    "    def __init__(self ):\n",
    "        self.means=np.array([0.485, 0.456, 0.406]) \n",
    "        self.stds=np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        im=image/255\n",
    "        im[:,:,0]=(image[:,:,0] - 0.485)/ 0.229 \n",
    "        im[:,:,1]=(image[:,:,1]  - self.means[1])/ self.stds[1]\n",
    "        im[:,:,2]=(image[:,:,2]  - self.means[2])/ self.stds[2]\n",
    "        image=im\n",
    "        return {'image': image, 'rating': rating}\n",
    "\n",
    "    \n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, rating = sample['image'], sample['rating']\n",
    "        \n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "            \n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image).double(),\n",
    "                'rating': torch.from_numpy(np.float64([rating])).double()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker=workers_test['worker'].unique()[0]\n",
    "num_images = workers_test[workers_test['worker'].isin([worker])].shape[0]\n",
    "percent=100/num_images\n",
    "images=workers_test[workers_test['worker'].isin([worker])][[' imagePair', ' score']]\n",
    "train_dataframe, valid_dataframe = train_test_split(images, train_size=percent)\n",
    "train_path=\"train_means_normalized\" + worker +\".csv\"\n",
    "test_path=\"test_means_normalized\" + worker +\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe.to_csv(train_path, sep=' ', index_label = False)\n",
    "valid_dataframe.to_csv(test_path, sep=' ', index_label = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size=(224,224)\n",
    "transformed_dataset_train = ImageRatingsDataset(csv_file=train_path,root_dir='Images/',\n",
    "                                           transform=transforms.Compose([Rescale(output_size=(256,256)),\n",
    "                                                                         RandomHorizontalFlip(0.5),\n",
    "                                                                         RandomCrop(output_size=output_size),\n",
    "                                                                         Normalize(),\n",
    "                                                                        ToTensor(),\n",
    "                                           ]))\n",
    "transformed_dataset_valid = ImageRatingsDataset(csv_file=test_path,root_dir='Images/',\n",
    "                                           transform=transforms.Compose([Rescale(output_size=(224,224)),\n",
    "                                                                         Normalize(),\n",
    "                                                                        ToTensor(),\n",
    "                                           ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import default_collate\n",
    "bsize=12\n",
    "def my_collate(batch):\n",
    "    batch = list(filter (lambda x:x is not None, batch))\n",
    "    return default_collate(batch)\n",
    "\n",
    "dataloader_train = DataLoader(transformed_dataset_train, batch_size=bsize,\n",
    "                        shuffle=True, num_workers=0,collate_fn=my_collate)\n",
    "dataloader_valid = DataLoader(transformed_dataset_valid, batch_size=8,\n",
    "                        shuffle=True, num_workers=0,collate_fn=my_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import copy\n",
    "from torch import nn\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "model_ft.fc = nn.Sequential(nn.Dropout(0.5),nn.Linear(num_ftrs,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, num_classes, keep_probability, inputsize):\n",
    "    \n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(inputsize, 256)\n",
    "        self.drop_prob = (1 - keep_probability)\n",
    "        self.relu1= nn.PReLU()\n",
    "        self.drop1 = nn.Dropout(self.drop_prob)\n",
    "        self.bn1=nn.BatchNorm1d(256)\n",
    "        self.fc2=nn.Linear(256,256)\n",
    "        self.relu2=nn.PReLU()\n",
    "        self.drop2 = nn.Dropout(p=self.drop_prob)\n",
    "        self.bn2=nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Weight initialization reference: https://arxiv.org/abs/1502.01852\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Feed-forward pass.\n",
    "        :param x: Input tensor\n",
    "        : return: Output tensor\n",
    "        \"\"\"\n",
    "        out=self.fc1(x)\n",
    "        out =self.relu1(out)\n",
    "        out=self.drop1(out)\n",
    "        out=self.bn1(out)\n",
    "        out=self.fc2(out)\n",
    "        out=self.relu2(out)\n",
    "        out=self.drop2(out)\n",
    "        out=self.bn2(out)\n",
    "        out=self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class convNet(nn.Module):\n",
    "    #constructor\n",
    "    def __init__(self,resnet,mynet):\n",
    "        super(convNet, self).__init__()\n",
    "        #defining layers in convnet\n",
    "        self.resnet=resnet\n",
    "        self.myNet=mynet\n",
    "    def forward(self, x):\n",
    "        x=self.resnet(x)\n",
    "        x=self.myNet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.out_features\n",
    "net1 = BaselineModel(1, 0.5,num_ftrs)\n",
    "net2=convNet(resnet=model_ft, mynet=net1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    cuda = torch.device('cuda:0')     # Default CUDA device\n",
    "count=0\n",
    "torch.cuda.set_device(cuda.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_lr_scheduler(optimizer, epoch, init_lr=0.0001, lr_decay_epoch=2):\n",
    "    \"\"\"Decay learning rate by a factor of DECAY_WEIGHT every lr_decay_epoch epochs.\"\"\"\n",
    "\n",
    "    if epoch % lr_decay_epoch == 0:\n",
    "        #print('LR is set to {}'.format(lr))\n",
    "        pass\n",
    "    lr = init_lr * (0.9**(epoch // lr_decay_epoch))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_2 = (torch.load('fine_tuned_flickerAES_normalized_dropout_resnet18_customnetworkadamnormalized.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in net_2.parameters():\n",
    "    param.requires_grad=True\n",
    "net_2.myNet.requires_grad=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def train_model(model, criterion, optimizer, lr_scheduler,dataloader_train,dataloader_valid,  num_epochs=100):\n",
    "    since = time.time()\n",
    "    train_loss_average=[]\n",
    "    test_loss=[]\n",
    "    best_model = model\n",
    "    best_loss = 100\n",
    "    use_gpu=True\n",
    "    criterion.cuda()\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader=dataloader_train\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "                dataloader=dataloader_valid\n",
    "\n",
    "            running_loss = 0.0\n",
    "            model.cuda()\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for batch_idx, data in enumerate(dataloader):\n",
    "                inputs = data['image']\n",
    "                labels=data['rating']\n",
    "                if use_gpu:\n",
    "                    try:\n",
    "                        inputs, labels = Variable(inputs.float().cuda()), Variable(labels.float().cuda())\n",
    "                    except:\n",
    "                        print(inputs,labels)\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                # wrap them in Variable\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                #print('loss done')                \n",
    "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
    "                #if counter%200==0:\n",
    "                    #print(\"Reached iteration \",counter)\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    #print('loss backward')\n",
    "                    loss.backward()\n",
    "                    #print('done loss backward')\n",
    "                    optimizer.step()\n",
    "                    #print('done optim')\n",
    "                # print evaluation statistics\n",
    "                try:\n",
    "                    running_loss += loss.data[0]\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "            #print('trying epoch loss')\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            #print('{} Loss: {:.4f} '.format(\n",
    "            #    phase, epoch_loss))\n",
    "            if phase == 'train':\n",
    "                train_loss_average.append(epoch_loss)\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                test_loss.append(epoch_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss=epoch_loss\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                    print('new best loss = ',epoch_loss)\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    print('returning and looping back')\n",
    "    return best_model.cuda(), train_loss_average, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "spearmanr(ratings_i, predictions_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSpearman(dataloader_valid, model):\n",
    "    ratings = []\n",
    "    predictions=[]\n",
    "    #device = cuda\n",
    "    #criterion = nn.MSELoss()\n",
    "    #criterion.cuda()\n",
    "    with torch.no_grad():\n",
    "        cum_loss=0\n",
    "        for batch_idx, data in enumerate(dataloader_valid):\n",
    "            inputs = data['image']\n",
    "            labels=data['rating']\n",
    "            if use_gpu:\n",
    "                try:\n",
    "                    inputs, labels = Variable(inputs.float().cuda()), Variable(labels.float().cuda())\n",
    "                except:\n",
    "                    print(inputs,labels)\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs = model(inputs)\n",
    "            ratings.append(labels.float())\n",
    "            predictions.append(outputs.float())\n",
    "    \n",
    "    ratings_i=[(list(np.float_([j for j in i]))) for i in ratings]\n",
    "    predictions_i=[(list(np.float_([j for j in i]))) for i in predictions]\n",
    "    ratings_i=np.vstack(ratings)\n",
    "    predictions_i=np.vstack(predictions)\n",
    "    from scipy.stats import spearmanr\n",
    "    return spearmanr(ratings_i, predictions_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ComputeIncreaseSP(sp):\n",
    "    return max(sp) - sp[0]\n",
    "\n",
    "def ComputeDecreaseMSE(MSEerrors):\n",
    "    return min(MSEerrors)-MSEerrors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "def train_model(model, criterion, optimizer, lr_scheduler,dataloader_train,dataloader_valid,  num_epochs=100):\n",
    "    since = time.time()\n",
    "    train_loss_average=[]\n",
    "    test_loss=[]\n",
    "    spearman_test=[]\n",
    "    best_model = model\n",
    "    best_loss = 100\n",
    "    best_spearman=0\n",
    "    use_gpu=True\n",
    "    criterion.cuda()\n",
    "    for epoch in range(num_epochs):\n",
    "        #print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        #print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['val', 'train']:\n",
    "            if phase == 'train':\n",
    "                mode='train'\n",
    "                optimizer = lr_scheduler(optimizer, epoch)\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader=dataloader_train\n",
    "            else:\n",
    "                model.eval()\n",
    "                mode='val'\n",
    "                dataloader=dataloader_valid\n",
    "\n",
    "            running_loss = 0.0\n",
    "            model.cuda()\n",
    "\n",
    "            counter=0\n",
    "            # Iterate over data.\n",
    "            for batch_idx, data in enumerate(dataloader):\n",
    "                inputs = data['image']\n",
    "                labels=data['rating']\n",
    "                if use_gpu:\n",
    "                    try:\n",
    "                        inputs, labels = Variable(inputs.float().cuda()), Variable(labels.float().cuda())\n",
    "                    except:\n",
    "                        print(inputs,labels)\n",
    "                else:\n",
    "                    inputs, labels = Variable(inputs), Variable(labels)\n",
    "                # wrap them in Variable\n",
    "\n",
    "                # Set gradient to zero to delete history of computations in previous epoch. Track operations so that differentiation can be done automatically.\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                #print('loss done')                \n",
    "                # Just so that you can keep track that something's happening and don't feel like the program isn't running.\n",
    "                #if counter%200==0:\n",
    "                    #print(\"Reached iteration \",counter)\n",
    "                counter+=1\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    #print('loss backward')\n",
    "                    loss.backward()\n",
    "                    #print('done loss backward')\n",
    "                    optimizer.step()\n",
    "                    #print('done optim')\n",
    "                # print evaluation statistics\n",
    "                try:\n",
    "                    running_loss += loss.data[0]\n",
    "                except:\n",
    "                    print('unexpected error, could not calculate loss or do a sum.')\n",
    "            #print('trying epoch loss')\n",
    "            epoch_loss = running_loss / len(dataloader)\n",
    "            #print('{} Loss: {:.4f} '.format(\n",
    "            #    phase, epoch_loss))\n",
    "            if phase == 'train':\n",
    "                train_loss_average.append(epoch_loss)\n",
    "\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                test_loss.append(epoch_loss)\n",
    "                sp=computeSpearman(dataloader, model)[0]\n",
    "                spearman_test.append(sp)\n",
    "\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss=epoch_loss\n",
    "                    #print('new best loss = ',epoch_loss)\n",
    "                \n",
    "                if sp>best_spearman:\n",
    "                    best_spearman=sp\n",
    "                    best_model = copy.deepcopy(model)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    print('returning and looping back')\n",
    "    return best_model.cuda(), train_loss_average, np.asarray(test_loss), np.asarray(spearman_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model_ft=net2\n",
    "\n",
    "\n",
    "#device = cuda\n",
    "criterion = nn.MSELoss()\n",
    "criterion.cuda()\n",
    "model_ft.cuda()\n",
    "optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ComputeDecreaseMSE(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(['worker', 'DecreaseMSE', 'IncreaseSP'])\n",
    "results=pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=worker, ComputeDecreaseMSE(test_loss), ComputeIncreaseSP(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=(np.asarray(results)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['MSE',0]=[ComputeIncreaseSP(sp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "stats=[]\n",
    "for worker_idx in range(0,37):\n",
    "    resu= []\n",
    "    for i in range(0,3):\n",
    "        worker=workers_test['worker'].unique()[worker_idx]\n",
    "        print(worker, i)\n",
    "        num_images = workers_test[workers_test['worker'].isin([worker])].shape[0]\n",
    "        percent=100/num_images\n",
    "        images=workers_test[workers_test['worker'].isin([worker])][[' imagePair', ' score']]\n",
    "        train_dataframe, valid_dataframe = train_test_split(images, train_size=percent)\n",
    "        train_path=\"train_means_normalized\" + worker +\".csv\"\n",
    "        test_path=\"test_means_normalized\" + worker +\".csv\"\n",
    "        train_dataframe.to_csv(train_path, sep=' ', index_label = False)\n",
    "        valid_dataframe.to_csv(test_path, sep=' ', index_label = False)\n",
    "\n",
    "        output_size=(224,224)\n",
    "        transformed_dataset_train = ImageRatingsDataset(csv_file=train_path,root_dir='Images/',\n",
    "                                               transform=transforms.Compose([Rescale(output_size=(256,256)),\n",
    "                                                                             RandomHorizontalFlip(0.5),\n",
    "                                                                             RandomCrop(output_size=output_size),\n",
    "                                                                             Normalize(),\n",
    "                                                                            ToTensor(),\n",
    "                                               ]))\n",
    "        transformed_dataset_valid = ImageRatingsDataset(csv_file=test_path,root_dir='Images/',\n",
    "                                               transform=transforms.Compose([Rescale(output_size=(224,224)),\n",
    "                                                                             Normalize(),\n",
    "                                                                            ToTensor(),\n",
    "                                               ]))\n",
    "\n",
    "        from torch.utils.data.dataloader import default_collate\n",
    "        bsize=30\n",
    "        def my_collate(batch):\n",
    "            batch = list(filter (lambda x:x is not None, batch))\n",
    "            return default_collate(batch)\n",
    "\n",
    "        dataloader_train = DataLoader(transformed_dataset_train, batch_size=bsize,\n",
    "                                shuffle=True, num_workers=0,collate_fn=my_collate)\n",
    "        dataloader_valid = DataLoader(transformed_dataset_valid, batch_size=20,\n",
    "                                shuffle=True, num_workers=0,collate_fn=my_collate)\n",
    "\n",
    "\n",
    "        model_ft= (torch.load('fine_tuned_flickerAES_normalized_dropout_resnet18_customnetworkadamnormalized.pt'))\n",
    "\n",
    "\n",
    "\n",
    "        #device = cuda\n",
    "        criterion = nn.MSELoss()\n",
    "        criterion.cuda()\n",
    "        model_ft.cuda()\n",
    "        optimizer = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "        iteration=0\n",
    "        network_path= worker + 'network_0 K100full it' + str(i) + '.pt'\n",
    "        # Run the functions and save the best model in the function model_ft.\n",
    "        model_ft, train_loss, test_loss, spearman = train_model(model_ft, criterion, optimizer, exp_lr_scheduler,dataloader_train,dataloader_valid,\n",
    "                               num_epochs=epochs)\n",
    " \n",
    "\n",
    "        #torch.save(model_ft, network_path)\n",
    "        results=worker, ComputeDecreaseMSE(test_loss), ComputeIncreaseSP(spearman), np.argmin(test_loss), np.argmax(spearman)\n",
    "        resu.append(results)\n",
    "        \n",
    "        \n",
    "    resupd=pd.DataFrame(resu)\n",
    "    resupd.columns=['Worker', 'DecreaseMSE', 'IncreaseSpearman', 'ItMSE', 'ItSpearman']\n",
    "    path=worker+'k100 finetuning all network.csv'\n",
    "    resupd.to_csv(path, sep=' ', index_label = False)\n",
    "    number=images[' imagePair'].unique().shape[0]\n",
    "    stat=resupd['Worker'][0],number, np.mean(resupd['DecreaseMSE']), np.std(resupd['DecreaseMSE']), np.mean(resupd['IncreaseSpearman']), np.std(resupd['IncreaseSpearman']), np.mean(resupd['ItMSE']), np.std(resupd['ItMSE']),np.mean(resupd['ItSpearman']), np.std(resupd['ItSpearman']),\n",
    "    stats.append(stat)\n",
    "stats=pd.DataFrame(stats)\n",
    "stats.to_csv('20 epochs k100 all layers.csv', sep=' ', index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
